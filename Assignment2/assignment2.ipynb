{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBB - Assignment #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install -r yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haarcascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "image_paths = []\n",
    "image_annotations = [] # center x, center y, width, height\n",
    "folder = os.path.join(os.getcwd(), \"ear_data\", \"test\")\n",
    "\n",
    "for filename in os.listdir(folder):\n",
    "        image = cv2.imread(os.path.join(folder,filename))\n",
    "        if image is not None:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            image_list.append(image)\n",
    "            image_paths.append(filename)\n",
    "        elif image is None:\n",
    "            txt = open(os.path.join(folder,filename), 'r')\n",
    "            line = txt.readline()\n",
    "            line = line.strip() # remove leading/trailing white spaces\n",
    "            annotations = [float(item.strip()) for item in line.split(' ')]\n",
    "            image_annotations.append(annotations)\n",
    "            txt.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cataf\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    }
   ],
   "source": [
    "ear_df = pd.DataFrame(image_list, columns=['Image'])\n",
    "ear_df.insert(1, \"Image label\",image_paths, False)\n",
    "ear_df.insert(2, \"Image annotations\",image_annotations, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_ears(img, l_cascade, r_cascade):\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "\n",
    "    ear_rect_l = l_cascade.detectMultiScale(img)\n",
    "    ear_rect_r = r_cascade.detectMultiScale(img)\n",
    "    \n",
    "    if len(ear_rect_l) != 0:\n",
    "        predictions = ear_rect_l[0]\n",
    "        predictions_processed = [float(predictions[0]/width), float(predictions[1]/height), float(predictions[2]/width), float(predictions[3]/height)]\n",
    "    elif len(ear_rect_r) != 0:\n",
    "        predictions = ear_rect_r[0]\n",
    "        predictions_processed = [float(predictions[0]/width), float(predictions[1]/height), float(predictions[2]/width), float(predictions[3]/height)]\n",
    "    else:\n",
    "        predictions = (None,None,None,None)\n",
    "        predictions_processed = [0,0,0,0]\n",
    "\n",
    "    return predictions_processed\n",
    "\n",
    "\n",
    "def get_accuracy(predictions):\n",
    "    final_acc = []\n",
    "    for index, values in enumerate(predictions):\n",
    "        accuracies = []\n",
    "        for i, v in enumerate(values):\n",
    "            if v != 0:\n",
    "                intermediate_accuracy = 1-abs(v-image_annotations[index][i+1])\n",
    "            else:\n",
    "                intermediate_accuracy = 0\n",
    "            accuracies.append(intermediate_accuracy)\n",
    "        final_acc.append(np.mean(accuracies))\n",
    "    return final_acc # np.mean(final_acc)\n",
    "\n",
    "\n",
    "def test_detect_ears_parameters(img, l_cascade, r_cascade, scales, neighbours):\n",
    "    height = img.shape[0]\n",
    "    width = img.shape[1]\n",
    "\n",
    "    # intermediate_image_predictions = []\n",
    "    image_predictions = []\n",
    "    \n",
    "    for scale in scales:\n",
    "        for neighbour in neighbours:\n",
    "            images_predictions = []\n",
    "            for img in image_list:\n",
    "                ear_rect_l = l_cascade.detectMultiScale(img, scale, neighbour)\n",
    "                ear_rect_r = r_cascade.detectMultiScale(img, scale, neighbour)\n",
    "\n",
    "                if len(ear_rect_l) != 0:\n",
    "                    predictions = ear_rect_l[0]\n",
    "                    predictions_processed = [float(predictions[0]/width), float(predictions[1]/height), float(predictions[2]/width), float(predictions[3]/height)]\n",
    "                elif len(ear_rect_r) != 0:\n",
    "                    predictions = ear_rect_r[0]\n",
    "                    predictions_processed = [float(predictions[0]/width), float(predictions[1]/height), float(predictions[2]/width), float(predictions[3]/height)]\n",
    "                else:\n",
    "                    predictions = (None,None,None,None)\n",
    "                    predictions_processed = [0,0,0,0]\n",
    "\n",
    "                images_predictions.append(predictions_processed)\n",
    "            image_predictions.append(images_predictions)\n",
    "\n",
    "    return image_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cascade classifiers for ears\n",
    "left_ear_cascade = cv2.CascadeClassifier('haarcascade_mcs_leftear.xml')\n",
    "right_ear_cascade = cv2.CascadeClassifier('haarcascade_mcs_rightear.xml')\n",
    "\n",
    "# get predictions\n",
    "predictions_vj = []\n",
    "for i in image_list:\n",
    "    predictions_vj.append(detect_ears(i, left_ear_cascade, right_ear_cascade))\n",
    "\n",
    "# get accuracy of predictions\n",
    "vj_accuracy_list = get_accuracy(predictions_vj)\n",
    "vj_accuracy = np.mean(vj_accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test parameters to fine-tune predictions\n",
    "scales = [1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2]\n",
    "neighbours = [0, 1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning predictions\n",
    "fine_tune_runs_vj = test_detect_ears_parameters(i, left_ear_cascade, right_ear_cascade, scales, neighbours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "# with open(\"predictions_vj_ft.pickle\",\"wb\") as f:\n",
    "#    pickle.dump(fine_tune_runs_vj, f)\n",
    "\n",
    "# Load dataframe\n",
    "with open(\"fine_tune_runs_vj.pickle\",\"rb\") as f:\n",
    "    fine_tune_runs_vj = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_df.insert(3, \"VJ\",predictions_vj, True)\n",
    "ear_df.insert(4, \"VJ Accuracy\", vj_accuracy_list, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe\n",
    "# with open(\"ear_df.pickle\",\"wb\") as f:\n",
    "#    pickle.dump(ear_df, f)\n",
    "\n",
    "# Load dataframe\n",
    "with open(\"ear_df.pickle\",\"rb\") as f:\n",
    "    ear_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column names \n",
    "column_names = []\n",
    "name_base = \"VJ \"\n",
    "for s in scales:\n",
    "    for n in neighbours:\n",
    "        column_names.append(name_base + \"S=\" + str(s) + \" N=\" + str(n))\n",
    "\n",
    "# Insert predictions in dataframe\n",
    "for i in range(0, len(fine_tune_runs_vj)): # list of len 70\n",
    "    for j in range(len(fine_tune_runs_vj[i])):\n",
    "        if fine_tune_runs_vj[i][j][0] == None: fine_tune_runs_vj[i][j][0] = 0\n",
    "        if fine_tune_runs_vj[i][j][1] == None: fine_tune_runs_vj[i][j][1] = 0\n",
    "        if fine_tune_runs_vj[i][j][2] == None: fine_tune_runs_vj[i][j][2] = 0\n",
    "        if fine_tune_runs_vj[i][j][3] == None: fine_tune_runs_vj[i][j][3] = 0\n",
    "    del ear_df[column_names[i]]\n",
    "    ear_df.insert(5+i, column_names[i] ,fine_tune_runs_vj[i], False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_IoU(predictions):\n",
    "    intersection_areas = []\n",
    "    union_areas = []\n",
    "    \n",
    "    for index, values in enumerate(predictions):\n",
    "        intersection_areas.append(max(0, -abs(predictions[index][0] - image_annotations[index][1]) + 1) * max(\n",
    "            0, -abs(predictions[index][1] - image_annotations[index][2]) + 1) if values[0] != 0 else 0)\n",
    "\n",
    "    union_areas = [2-inter if predictions[index][0] !=\n",
    "                   0 else 2 for index, inter in enumerate(intersection_areas)]\n",
    "\n",
    "    # IoU list for all images\n",
    "    IoU = [i/u for _,\n",
    "           (i, u) in enumerate(zip(intersection_areas, union_areas))]\n",
    "\n",
    "    return IoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert all image accuracies and IoUs into new dataframe\n",
    "acc_column_name_list = []\n",
    "IoU_column_name_list = []\n",
    "column_accuracies = []\n",
    "column_IoUs = []\n",
    "total_IoUs = []\n",
    "\n",
    "for c in column_names:\n",
    "    column_acc_results = get_accuracy(ear_df[c])\n",
    "    column_acc = np.mean(column_acc_results)\n",
    "    column_accuracies.append(column_acc)\n",
    "    acc_column_name = \"Acc \" + c\n",
    "    acc_column_name_list.append(acc_column_name)\n",
    "    del ear_df[acc_column_name]\n",
    "    ear_df.insert(len(ear_df.columns), acc_column_name, column_acc_results, False)\n",
    "\n",
    "    IoU_column_name = \"IoU \" + c\n",
    "    IoU_column_name_list.append(IoU_column_name)\n",
    "    column_IoU_results = get_IoU(ear_df[c])\n",
    "    total_IoUs.extend(column_IoU_results)\n",
    "    column_IoU_mean = np.mean(column_IoU_results)\n",
    "    column_IoUs.append(column_IoU_mean)\n",
    "    del ear_df[IoU_column_name]\n",
    "    ear_df.insert(len(ear_df.columns), IoU_column_name, column_IoU_results, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_params_df = pd.DataFrame(column_names, columns=['Image parameters'])\n",
    "ear_params_df.insert(1, \"Accuracies\",column_accuracies, False)\n",
    "ear_params_df.insert(2, \"IoUs\",column_IoUs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe\n",
    "# with open(\"ear_params_df.pickle\",\"wb\") as f:\n",
    "#    pickle.dump(ear_params_df, f)\n",
    "\n",
    "# Load dataframe\n",
    "with open(\"ear_params_df.pickle\",\"rb\") as f:\n",
    "    ear_params_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image parameters</th>\n",
       "      <th>Accuracies</th>\n",
       "      <th>IoUs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VJ S=1.1 N=0</td>\n",
       "      <td>0.783817</td>\n",
       "      <td>0.493783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VJ S=1.2 N=0</td>\n",
       "      <td>0.726743</td>\n",
       "      <td>0.465292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>VJ S=1.3 N=0</td>\n",
       "      <td>0.674724</td>\n",
       "      <td>0.439796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>VJ S=1.4 N=0</td>\n",
       "      <td>0.635800</td>\n",
       "      <td>0.417217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>VJ S=1.5 N=0</td>\n",
       "      <td>0.625254</td>\n",
       "      <td>0.410071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Image parameters  Accuracies      IoUs\n",
       "0      VJ S=1.1 N=0    0.783817  0.493783\n",
       "7      VJ S=1.2 N=0    0.726743  0.465292\n",
       "14     VJ S=1.3 N=0    0.674724  0.439796\n",
       "21     VJ S=1.4 N=0    0.635800  0.417217\n",
       "28     VJ S=1.5 N=0    0.625254  0.410071"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 5 best results based on accuracy\n",
    "ear_params_df.sort_values(by=['Accuracies'], ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image parameters</th>\n",
       "      <th>Accuracies</th>\n",
       "      <th>IoUs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>VJ S=2 N=6</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.016354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>VJ S=1.8 N=6</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.019480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>VJ S=1.9 N=6</td>\n",
       "      <td>0.023132</td>\n",
       "      <td>0.019473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>VJ S=1.4 N=6</td>\n",
       "      <td>0.026978</td>\n",
       "      <td>0.023258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>VJ S=1.7 N=6</td>\n",
       "      <td>0.028924</td>\n",
       "      <td>0.024869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Image parameters  Accuracies      IoUs\n",
       "69       VJ S=2 N=6    0.019200  0.016354\n",
       "55     VJ S=1.8 N=6    0.023048  0.019480\n",
       "62     VJ S=1.9 N=6    0.023132  0.019473\n",
       "27     VJ S=1.4 N=6    0.026978  0.023258\n",
       "48     VJ S=1.7 N=6    0.028924  0.024869"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 5 worst results based on accuracy\n",
    "ear_params_df.sort_values(by=['Accuracies'], ascending=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_graphs(results, best_or_worst):\n",
    "    counter = 1\n",
    "    for result in results:    \n",
    "        precisions = []\n",
    "        for t in range(99):\n",
    "            threshold = (t+1)/100\n",
    "            precisions.append(len([i for i in ear_df[\"IoU \" + result[0]] if i > threshold])/500) # IoU list has 500 elements\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.canvas.draw()\n",
    "        plt.plot(precisions)\n",
    "\n",
    "        labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "        labels = [0,0,0.2,0.4,0.6,0.8,1]\n",
    "        plt.title(\"Precision-Recall Haarcascades\")\n",
    "        ax.set_xticklabels(labels)\n",
    "        plt.savefig(\"Precision-Recall-Haarcascades-\" + best_or_worst + \"-\" +str(counter))\n",
    "        plt.show()\n",
    "\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction_and_gt(img,index,prediction_ear,gt):\n",
    "\n",
    "    ear_img = img.copy()\n",
    "    if prediction_ear != [0, 0, 0, 0]:\n",
    "        cv2.rectangle(ear_img, (int(prediction_ear[0]*ear_img.shape[1]), int(prediction_ear[1]*ear_img.shape[0])), (int((prediction_ear[0] + prediction_ear[2])*ear_img.shape[1]), int((prediction_ear[1] +prediction_ear[3])*ear_img.shape[0])), (255, 0, 0), 6)\n",
    "        cv2.rectangle(ear_img, (int(gt[1]*ear_img.shape[1]), int(gt[2]*ear_img.shape[0])), (int((gt[1] + gt[3])*ear_img.shape[1]), int((gt[2] + gt[4])*ear_img.shape[0])), (255, 255, 255), 2)\n",
    "        cv2.imshow(ear_df[\"Image label\"][index], ear_img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    return ear_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall for the baseline VJ\n",
    "get_pr_graphs([[\"VJ S=1.1 N=3\"]] ,\"Baseline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall for the 5 parameter combinations with the best accuracy\n",
    "top_5_results = ear_params_df.sort_values(by=['Accuracies'], ascending=False)[:5].values\n",
    "get_pr_graphs(top_5_results, \"Best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall for the 5 parameter combinations with the worst accuracy\n",
    "bottom_5_results = ear_params_df.sort_values(by=['Accuracies'], ascending=True)[:5].values\n",
    "get_pr_graphs(bottom_5_results, \"Worst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 5 best results based on the best parameter combinations\n",
    "ear_df.sort_values(by=[\"Acc VJ S=1.1 N=0\"], ascending=False)[:5]\n",
    "\n",
    "# 89     0.985078 0590.png\n",
    "# 163    0.982411 0664.png\n",
    "# 220    0.981269 1921.png\n",
    "# 229    0.981180 1930.png\n",
    "# 226    0.980142 1927.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_best_match_1 = draw_prediction_and_gt(image_list[89], 89, ear_df[\"VJ S=1.1 N=0\"][89], image_annotations[89])\n",
    "image_best_match_2 = draw_prediction_and_gt(image_list[163], 163, ear_df[\"VJ S=1.1 N=0\"][163], image_annotations[163])\n",
    "image_best_match_3 = draw_prediction_and_gt(image_list[220], 18, ear_df[\"VJ S=1.1 N=0\"][220], image_annotations[220])\n",
    "image_best_match_4 = draw_prediction_and_gt(image_list[229], 18, ear_df[\"VJ S=1.1 N=0\"][229], image_annotations[229])\n",
    "image_best_match_5 = draw_prediction_and_gt(image_list[226], 18, ear_df[\"VJ S=1.1 N=0\"][226], image_annotations[226])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall for the 10 worst accuracies\n",
    "bottom_10_results = ear_params_df.sort_values(by=['Accuracies'], ascending=True)[:10].values\n",
    "get_pr_graphs(bottom_10_results, \"Worst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_worst_match_1 = draw_prediction_and_gt(image_list[8], 8, ear_df[\"VJ S=2 N=6\"][8], image_annotations[8])\n",
    "image_worst_match_2 = draw_prediction_and_gt(image_list[35], 35, ear_df[\"VJ S=2 N=6\"][35], image_annotations[35])\n",
    "image_worst_match_3 = draw_prediction_and_gt(image_list[78], 78, ear_df[\"VJ S=2 N=6\"][78], image_annotations[78])\n",
    "image_worst_match_4 = draw_prediction_and_gt(image_list[101], 101, ear_df[\"VJ S=2 N=6\"][101], image_annotations[101])\n",
    "image_worst_match_5 = draw_prediction_and_gt(image_list[138], 138, ear_df[\"VJ S=2 N=6\"][138], image_annotations[138])\n",
    "image_worst_match_6 = draw_prediction_and_gt(image_list[152], 152, ear_df[\"VJ S=2 N=6\"][152], image_annotations[152])\n",
    "image_worst_match_7 = draw_prediction_and_gt(image_list[276], 276, ear_df[\"VJ S=2 N=6\"][276], image_annotations[276])\n",
    "image_worst_match_8 = draw_prediction_and_gt(image_list[334], 334, ear_df[\"VJ S=2 N=6\"][334], image_annotations[334])\n",
    "image_worst_match_9 = draw_prediction_and_gt(image_list[357], 357, ear_df[\"VJ S=2 N=6\"][357], image_annotations[357])\n",
    "image_worst_match_10 = draw_prediction_and_gt(image_list[18], 18, ear_df[\"VJ S=1.8 N=6\"][18], image_annotations[18])\n",
    "\n",
    "# Mismatches:\n",
    "# 8 S=2 N=6\n",
    "# 35 ... \n",
    "# 78\n",
    "# 101\n",
    "# 138\n",
    "# 152\n",
    "# 276\n",
    "# 334\n",
    "# 357\n",
    "# 18 S=1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "image_match = draw_prediction_and_gt(image_list[18], ear_df[\"VJ S=1.1 N=0\"][18], image_annotations[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5_model = torch.hub.load(\n",
    "    \"yolov5\", 'custom', path=\"yolo5s.pt\", source='local')\n",
    "\n",
    "img = 'ear_data/test/0501.png'\n",
    "yolov5_results = yolov5_model(img)\n",
    "# Results, change the flowing to: results.show()\n",
    "yolov5_results.show()  # or .show(), .save(), .crop(), .pandas(), etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5_results_1st_fifth = yolov5_model(image_list[0:100])\n",
    "yolov5_results_2nd_fifth = yolov5_model(image_list[100:200])\n",
    "yolov5_results_3rd_fifth = yolov5_model(image_list[200:300])\n",
    "yolov5_results_4th_fifth = yolov5_model(image_list[300:400])\n",
    "yolov5_results_5th_fifth = yolov5_model(image_list[400:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5_1st_fifth_pos = yolov5_results_1st_fifth.pandas().xyxy\n",
    "yolov5_2nd_fifth_pos = yolov5_results_2nd_fifth.pandas().xyxy\n",
    "yolov5_3rd_fifth_pos = yolov5_results_3rd_fifth.pandas().xyxy\n",
    "yolov5_4th_fifth_pos = yolov5_results_4th_fifth.pandas().xyxy\n",
    "yolov5_5th_fifth_pos = yolov5_results_5th_fifth.pandas().xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize yolo results [0, 1] \n",
    "def normalize_yolo_results(starting_index, yolov5_pos):\n",
    "    yolov5_norm_results = []\n",
    "    for index, r in enumerate(yolov5_pos):\n",
    "        index += starting_index\n",
    "        if not r[\"class\"].empty:\n",
    "            yolov5_norm_results.append(\n",
    "                [r[\"xmin\"].values[0]/image_list[index].shape[1], r[\"ymin\"].values[0]/image_list[index].shape[0], r[\"xmax\"].values[0]/image_list[index].shape[1], r[\"ymax\"].values[0]/image_list[index].shape[0]])\n",
    "            continue\n",
    "        yolov5_norm_results.append([0, 0, 0, 0])\n",
    "    return yolov5_norm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5_1st_fifth_pos_normalized = normalize_yolo_results(0, yolov5_1st_fifth_pos)\n",
    "yolov5_2nd_fifth_pos_normalized = normalize_yolo_results(100, yolov5_2nd_fifth_pos)\n",
    "yolov5_3rd_fifth_pos_normalized = normalize_yolo_results(200, yolov5_3rd_fifth_pos)\n",
    "yolov5_4th_fifth_pos_normalized = normalize_yolo_results(300, yolov5_4th_fifth_pos)\n",
    "yolov5_5th_fifth_pos_normalized = normalize_yolo_results(400, yolov5_5th_fifth_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_normalized = []\n",
    "all_pos_normalized.extend(yolov5_1st_fifth_pos_normalized)\n",
    "all_pos_normalized.extend(yolov5_2nd_fifth_pos_normalized)\n",
    "all_pos_normalized.extend(yolov5_3rd_fifth_pos_normalized)\n",
    "all_pos_normalized.extend(yolov5_4th_fifth_pos_normalized)\n",
    "all_pos_normalized.extend(yolov5_5th_fifth_pos_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe\n",
    "# with open(\"all_pos_normalized.pickle\",\"wb\") as f:\n",
    "#    pickle.dump(all_pos_normalized, f)\n",
    "\n",
    "# Load dataframe\n",
    "with open(\"all_pos_normalized.pickle\",\"rb\") as f:\n",
    "    all_pos_normalized = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5_accuracies = []\n",
    "yolov5_accuracies_mean = []\n",
    "\n",
    "for index, pos in enumerate(all_pos_normalized):\n",
    "    yolov5_accuracies.append(np.mean([1-abs(v-image_annotations[index][i+1]) if v != 0 else 0 for i, v in enumerate(pos)]))\n",
    "\n",
    "yolov5_accuracies_mean.append(np.mean(yolov5_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5_IoU_list = get_IoU(all_pos_normalized)\n",
    "mean_yolov5_IoU = np.mean(yolov5_IoU_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del ear_df[\"Acc YOLOv5\"]\n",
    "# del ear_df[\"IoU YOLOv5\"]\n",
    "ear_df.insert(len(ear_df.columns), \"Acc YOLOv5\", yolov5_accuracies, False)\n",
    "ear_df.insert(len(ear_df.columns), \"IoU YOLOv5\", yolov5_IoU_list, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add YOLOv5 row\n",
    "ear_params_df = ear_params_df.append({\"Image parameters\": \"YOLOv5\", \"Accuracies\": yolov5_accuracies_mean[0], \"IoUs\": mean_yolov5_IoU}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best YOLOv5 predictions based on accuracy\n",
    "ear_df.sort_values(by=['Acc YOLOv5'], ascending=False)[:5][\"Image label\"]\n",
    "\n",
    "# 32     0.930307 0533.png\n",
    "# 430    0.906633 2131.png\n",
    "# 362    0.904298 2063.png\n",
    "# 148    0.895469 0649.png\n",
    "# 3      0.891080 0504.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pr_graphs_yolo(results, best_or_worst):\n",
    "    counter = 1\n",
    "   \n",
    "    precisions = []\n",
    "    for t in range(99):\n",
    "        threshold = (t+1)/100\n",
    "        precisions.append(len([i for i in results if i > threshold])/500) # IoU list has 500 elements\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.canvas.draw()\n",
    "    plt.plot(precisions)\n",
    "\n",
    "    labels = [item.get_text() for item in ax.get_xticklabels()]\n",
    "    labels = [0,0,0.2,0.4,0.6,0.8,1]\n",
    "    plt.title(\"Precision-Recall YOLOv5\")\n",
    "    ax.set_xticklabels(labels)\n",
    "    plt.savefig(\"Precision-Recall-YOLOv5-\" + best_or_worst + \"-\" +str(counter))\n",
    "    plt.show()\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pr_graphs_yolo(yolov5_IoU_list, \"Base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_img_1_yolo = 'ear_data/test/0533.png'\n",
    "best_img_2_yolo = 'ear_data/test/2131.png'\n",
    "best_img_3_yolo = 'ear_data/test/2063.png'\n",
    "best_img_4_yolo = 'ear_data/test/0649.png'\n",
    "best_img_5_yolo = 'ear_data/test/0504.png'\n",
    "\n",
    "yolov5_best_result_1 = yolov5_model(best_img_1_yolo)\n",
    "yolov5_best_result_2 = yolov5_model(best_img_2_yolo)\n",
    "yolov5_best_result_3 = yolov5_model(best_img_3_yolo)\n",
    "yolov5_best_result_4 = yolov5_model(best_img_4_yolo)\n",
    "yolov5_best_result_5 = yolov5_model(best_img_5_yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5_best_result_1.show()\n",
    "yolov5_best_result_2.show()\n",
    "yolov5_best_result_3.show()\n",
    "yolov5_best_result_4.show()\n",
    "yolov5_best_result_5.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4fcfa26c52f17e4de6d75c06366f2bd56da70157074da99079e7df3021447de2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
